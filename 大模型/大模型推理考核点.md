### Q1：RMSNorm 相比较 LayerNorm为什么可以减少参数量？原理是什么？
相比较LayerNorm, RMSNorm 通过省略了可学习参数：平移参数 $\beta$, 将参数量从2d降低到d（d为输入x向量的维度）, 从而减少了模型的复杂度和计算开销。同时，大模型实验表明，性能相比前者相当甚至更好。  

**LayerNorm具体公式如下**：  
$$Layernorm(x) = \gamma * \frac {x - \mu}{\sigma} + \beta$$ 

*P.S. 其中d个参数用于缩放，d个参数用于平移*  

**RMSNorm具体公式如下**：  
$$RMSNorm(x) = \gamma * \frac {x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}}$$

*P.S. 其中d个参数用于缩放*  


### Q2：请介绍下什么是大模型的外推性？
大模型的外推可以理解为：相比较训练过程使用的训练文本token长度，大模型在推理时，其能适应更长序列文本输入（如多轮对话场景），能够对于超过上下文窗口更远距离的词元进行有效建模，推理性能依旧。比如，训练过程使用的文本token长度是512, 在推理过程输入的文本token长度为1024，其推理效果并未出现明显下降。


### Q3：请介绍下什么是大模型位置编码的几种形式？
+ **绝对位置编码**  
  + Sinusoidal Eencoding（正旋和余旋编码）
    + 公式如下：  
    $PE(pos, 2i) = sin(\frac{pos}{{10000}^{\frac{2i}{d}}})$  
    $PE(pos, 2i+1) = cos(\frac{pos}{{10000}^{\frac{2i}{d}}})$  
    *其中，pos: sequence idx; i: 词嵌入特征维度idx; 词嵌入特征的维度*。
    + 优点：在词嵌入向量中加入绝对位置编码信息，transformer中引入了绝对位置信息，关注了序列的词信息。
    + 缺点：未考虑相对位置信息，不具备外推性。
  + 可学习参数
    + 替换正旋和余旋编码，通过模型学习得到，优缺点同上。
+ **相对位置编码**
    + 相对位置编码通常应用于注意力矩阵的计算中，而不是直接与词元嵌入向量进行相加。
    + 代表算法：Transformer-XL，T5（引入可学习的相对位置标量表征Q，K的相对距离）。
    + 优点：已经具备一定的外推性。
+ **RoPE旋转位置编码**
    + 通过基于绝对位置的旋转矩阵来表示注意力机制中的Q，K相对位置信息。并论证了：*位置索引i的旋转矩阵和位置索引j的旋转矩阵的乘积=相对距离的i-j的旋转矩阵*。（二维向量->复平面表示->极坐标表示->旋转矩阵表示->推广到一般维度的向量）
    + 优点：注意力机制中引入，无需额外的位置编码向量，简单高效，无需显示位置编码。加入了相对位置信息，良好的外推性能以及长期衰减的特性
    + 代表算法：PaLM，LLaMA，DeepSeek
+ **Abili位置编码**
    + ALiBi 通过在键和查询之间的距离上施加相对距离相关的惩罚来调整注意力分数。其中，在多头注意力下(8头)，每个头会分配不同的值。$\frac{1}{2}$, $\frac{1}{2^2}$, $\frac{1}{2^3}$, $\frac{1}{2^4}$...$\frac{1}{2^8}$。  
    *P.S. i − j 是查询和键之间的位置偏移量，𝑚 是每个注意力头独有的惩罚系数*。
    + 优点：注意力机制中引入，无需额外的位置编码向量，简单高效，无需显示位置编码，ALiBi 展现出了优秀的外推性能，能够对于超过上下文窗口更远距离的词元进行有效建模。
    + 缺点：无法捕捉复杂的相对位置关系
