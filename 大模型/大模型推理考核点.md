### Q1：RMSNorm 相比较 LayerNorm为什么可以减少参数量？原理是什么？
相比较LayerNorm, RMSNorm 通过省略了可学习参数：平移参数 $\beta$, 将参数量从2d降低到d（d为输入x向量的维度）, 从而减少了模型的复杂度和计算开销。同时，大模型实验表明，性能相比前者相当甚至更好。  

**LayerNorm具体公式如下**：  
$$Layernorm(x) = \gamma * \frac {x - \mu}{\sigma} + \beta$$ 

*P.S. 其中d个参数用于缩放，d个参数用于平移*  

**RMSNorm具体公式如下**：  
$$RMSNorm(x) = \gamma * \frac {x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}}$$

*P.S. 其中d个参数用于缩放*  


### Q2：请介绍下什么是大模型的外推性？
大模型的外推可以理解为：相比较训练过程使用的训练文本token长度，大模型在推理时，其能适应更长序列文本输入（如多轮对话场景），能够对于超过上下文窗口更远距离的词元进行有效建模，推理性能依旧。比如，训练过程使用的文本token长度是512, 在推理过程输入的文本token长度为1024，其推理效果并未出现明显下降。